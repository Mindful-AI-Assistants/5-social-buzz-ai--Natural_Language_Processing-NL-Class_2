{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5vTZJWJxCN7"
      },
      "source": [
        "# üìì Notebook: Pipeline e Pr√©-processamento em PLN (Aula 1)\n",
        "\n",
        "## 1\\. Introdu√ß√£o ao Processamento de Linguagem Natural (PLN)\n",
        "\n",
        "O Processamento de Linguagem Natural (PLN), ou Natural Language Processing (NLP), √© um campo de estudo fundamental na intersec√ß√£o entre a ci√™ncia da computa√ß√£o, a intelig√™ncia artificial e a lingu√≠stica. Seu foco √© fazer com que os computadores processem, \"entendam\" e alavanquem dados de linguagem humana.\n",
        "\n",
        "O estudo do PLN √© motivado pelo imenso volume de dados de linguagem gerados diariamente, sendo a maior parte deles textuais. O ponto central da computa√ß√£o √© auxiliar os seres humanos; portanto, o PLN √© um passo obrigat√≥rio para que os computadores auxiliem significativamente nossas vidas, entendendo a forma como nos comunicamos.\n",
        "\n",
        "Os dois pilares do PLN s√£o:\n",
        "\n",
        "  * **Representa√ß√£o:** Focada em converter o significado simb√≥lico da linguagem (palavras, fala, sinais) em uma forma que o computador possa entender, geralmente dados num√©ricos, como vetores densos (embeddings).\n",
        "  * **Modelagem:** Envolve o uso dessas representa√ß√µes num√©ricas para executar tarefas de linguagem, o que hoje √© dominado por Redes Neurais Profundas.\n",
        "\n",
        "## 2\\. Configura√ß√£o do Ambiente e Pipeline Gen√©rica\n",
        "\n",
        "Antes de iniciar qualquer tarefa de PLN, definimos a pipeline gen√©rica, que √© conceitualmente dividida em fases distintas:\n",
        "\n",
        "1.  Aquisi√ß√£o de Dados\n",
        "2.  Limpeza (Text Cleaning)\n",
        "3.  Pr√©-processamento\n",
        "4.  Feature Engineering (Representa√ß√£o de Texto)\n",
        "5.  Modelagem\n",
        "6.  Avalia√ß√£o\n",
        "\n",
        "Neste Notebook, focaremos nas etapas 2 e 3: **Limpeza** e **Pr√©-processamento**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting nltk\n",
            "  Downloading nltk-3.9.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: click in /Users/rooneycoelho/miniforge3/lib/python3.12/site-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /Users/rooneycoelho/miniforge3/lib/python3.12/site-packages (from nltk) (1.5.1)\n",
            "Collecting regex>=2021.8.3 (from nltk)\n",
            "  Downloading regex-2025.10.23-cp312-cp312-macosx_11_0_arm64.whl.metadata (40 kB)\n",
            "Requirement already satisfied: tqdm in /Users/rooneycoelho/miniforge3/lib/python3.12/site-packages (from nltk) (4.67.1)\n",
            "Downloading nltk-3.9.2-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m444.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading regex-2025.10.23-cp312-cp312-macosx_11_0_arm64.whl (288 kB)\n",
            "Installing collected packages: regex, nltk\n",
            "Successfully installed nltk-3.9.2 regex-2025.10.23\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFv0UZz2xCN_",
        "outputId": "22532a4e-fbdf-4323-a42b-ebf47d88457a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Texto Bruto Adquirido:\n",
            " \n",
            "<doc>\n",
            "  <title>AULA 1 - Exemplo de PLN</title>\n",
            "  <text>\n",
            "    O PLN √© uma √É¬°rea cr√Ética! H√É¬° muitos dados textuais (e.g., TwEETS, coment√É¬°rios).\n",
            "    A tokeniza√É¬ß√É¬£o (tokenizer) √É¬© a primeira etapa. N√É¬£o se esque√É¬ßa de Lematiza√É¬ß√É¬£o.\n",
            "    √â F√ÅCIL SOBREAJUSTAR O MODELO (overfitting).\n",
            "  </text>\n",
            "</doc>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk # Usado para muitas tarefas de PLN, como tokeniza√ß√£o e stemming\n",
        "from nltk.corpus import stopwords\n",
        "# No ambiente real, usar√≠amos ferramentas como Spark NLP para pipelines complexas\n",
        "\n",
        "# Importa√ß√µes comuns para tarefas de pr√©-processamento\n",
        "\n",
        "# Simula√ß√£o de dados de texto que geralmente √© o ponto de partida\n",
        "texto_bruto = \"\"\"\n",
        "<doc>\n",
        "  <title>AULA 1 - Exemplo de PLN</title>\n",
        "  <text>\n",
        "    O PLN √© uma √É¬°rea cr√Ética! H√É¬° muitos dados textuais (e.g., TwEETS, coment√É¬°rios).\n",
        "    A tokeniza√É¬ß√É¬£o (tokenizer) √É¬© a primeira etapa. N√É¬£o se esque√É¬ßa de Lematiza√É¬ß√É¬£o.\n",
        "    √â F√ÅCIL SOBREAJUSTAR O MODELO (overfitting).\n",
        "  </text>\n",
        "</doc>\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 1. Etapa de Aquisi√ß√£o de Dados\n",
        "# Em projetos reais, envolve a coleta de datasets p√∫blicos ou rotulados.\n",
        "# Aqui, usamos uma string de exemplo que simula dados \"sujos\" extra√≠dos (scraped data).\n",
        "print(\"Texto Bruto Adquirido:\\n\", texto_bruto)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifGFgxV6xCOA"
      },
      "source": [
        "## 3\\. Etapa de Limpeza de Texto (Text Cleaning)\n",
        "\n",
        "A Limpeza de Texto √© cr√≠tica, pois afeta todas as fases subsequentes da pipeline. O foco √© a extra√ß√£o do texto bruto e a remo√ß√£o de informa√ß√µes n√£o textuais.\n",
        "\n",
        "Exemplos de tarefas de limpeza incluem:\n",
        "\n",
        "  * Remo√ß√£o de marca√ß√µes HTML/XML, metadados e artefatos de raspagem.\n",
        "  * Normaliza√ß√£o de Unicode e corre√ß√£o ortogr√°fica (crucial para dados de m√≠dias sociais).\n",
        "\n",
        "<!-- end list -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YtC-KkeexCOB",
        "outputId": "ccac8be0-924a-453d-f4c2-7979fc4aab54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Texto ap√≥s Limpeza e Normaliza√ß√£o de Unicode:\n",
            " AULA 1 - Exemplo de PLN\n",
            "  \n",
            "    O PLN √© uma √°rea cr√≠tica! H√° muitos dados textuais (e.g., TwEETS, coment√°rios).\n",
            "    A tokeniza√ß√£o (tokenizer) √© a primeira etapa. N√£o se esque√ßa de Lematiza√ß√£o.\n",
            "    √â F√ÅCIL SOBREAJUSTAR O MODELO (overfitting).\n"
          ]
        }
      ],
      "source": [
        "# 2. Etapa de Limpeza (Text Cleaning)\n",
        "\n",
        "# 2.1 Remo√ß√£o de Marca√ß√µes e Metadados\n",
        "# Remove tags XML/HTML.\n",
        "texto_limpo = re.sub(r'<[^>]+>', '', texto_bruto)\n",
        "texto_limpo = texto_limpo.strip()\n",
        "\n",
        "# 2.2 Normaliza√ß√£o de Unicode (Exemplo simples: substitui√ß√£o de acentos codificados incorretamente)\n",
        "# No exemplo, corrigimos codifica√ß√µes erradas comuns (√É¬° -> √°, √É¬ß -> √ß)\n",
        "texto_normalizado = texto_limpo.replace(\"√É¬°\", \"√°\").replace(\"√É¬£\", \"√£\").replace(\"√É¬©\", \"√©\").replace(\"√É¬ß\", \"√ß\").replace(\"√É\", \"√≠\")\n",
        "\n",
        "print(\"Texto ap√≥s Limpeza e Normaliza√ß√£o de Unicode:\\n\", texto_normalizado)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6t8HekhxxCOB"
      },
      "source": [
        "## 4\\. Etapa de Pr√©-processamento: Tokeniza√ß√£o e Segmenta√ß√£o\n",
        "\n",
        "Ap√≥s a limpeza, o Pr√©-processamento converte o texto em uma forma can√¥nica e estruturalmente utiliz√°vel, pois o software de PLN opera no n√≠vel de senten√ßa e palavra.\n",
        "\n",
        "### 4.1 Tokeniza√ß√£o\n",
        "\n",
        "A Tokeniza√ß√£o √© uma das t√©cnicas mais fundamentais e a primeira etapa crucial do pr√©-processamento. Consiste em dividir uma string de caracteres em peda√ßos menores, geralmente palavras, chamadas **tokens**. O PLN geralmente opera no n√≠vel de palavra, pois √© a menor unidade de significado conveniente.\n",
        "\n",
        "Embora em ingl√™s um tokenizer simples possa usar espa√ßos em branco, o processo √© afetado pelo sistema de escrita da linguagem e deve lidar com complexidades como pontua√ß√µes e abrevia√ß√µes (e.g., dividindo \"NLP.\" em \"NLP\" e \".\")."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Te1CF463xQ41",
        "outputId": "3a093ff4-0f9e-4ec3-bbe4-910ab3365e73"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /Users/rooneycoelho/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snGMHsJoxCOB",
        "outputId": "6c843496-3f64-42f4-ce91-8328d9c67b1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "N√∫mero total de tokens: 50\n",
            "\n",
            "Tokens (unidades at√¥micas de significado):\n",
            " ['AULA', '1', '-', 'Exemplo', 'de', 'PLN', 'O', 'PLN', '√©', 'uma', '√°rea', 'cr√≠tica', '!', 'H√°', 'muitos', 'dados', 'textuais', '(', 'e.g.', ',', 'TwEETS', ',', 'coment√°rios', ')', '.', 'A', 'tokeniza√ß√£o', '(', 'tokenizer', ')', '√©', 'a', 'primeira', 'etapa', '.', 'N√£o', 'se', 'esque√ßa', 'de', 'Lematiza√ß√£o', '.', '√â', 'F√ÅCIL', 'SOBREAJUSTAR', 'O', 'MODELO', '(', 'overfitting', ')', '.']\n"
          ]
        }
      ],
      "source": [
        "# 3. Etapa de Pr√©-processamento\n",
        "\n",
        "# A Segmenta√ß√£o de Senten√ßa (Sentence Segmentation) √© um preliminar essencial.\n",
        "# Usamos o NLTK para simular o processo.\n",
        "\n",
        "# Download dos recursos necess√°rios do NLTK (para tokeniza√ß√£o)\n",
        "# nltk.download('punkt')\n",
        "\n",
        "from nltk.tokenize import sent_tokenize\n",
        "# Nota: Para este exemplo, simplificaremos, mas sent_tokenize() realiza a separa√ß√£o de frases.\n",
        "\n",
        "# Tokeniza√ß√£o de Palavras (Word Tokenization)\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Tratamento do texto para Tokeniza√ß√£o (remo√ß√£o de quebras de linha/espa√ßos excessivos)\n",
        "texto_para_tokenizar = re.sub(r'\\s+', ' ', texto_normalizado)\n",
        "tokens = word_tokenize(texto_para_tokenizar)\n",
        "\n",
        "print(f\"N√∫mero total de tokens: {len(tokens)}\")\n",
        "print(\"\\nTokens (unidades at√¥micas de significado):\\n\", tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DU22QOqKxCOB"
      },
      "source": [
        "## 5\\. Etapa de Pr√©-processamento: Redu√ß√£o de Vocabul√°rio e Normaliza√ß√£o\n",
        "\n",
        "O Pr√©-processamento tamb√©m inclui a **Redu√ß√£o de Vocabul√°rio**, visando agrupar formas flexionadas da mesma palavra em uma √∫nica forma can√¥nica, al√©m de outras pr√°ticas de normaliza√ß√£o.\n",
        "\n",
        "### 5.1 Redu√ß√£o de Vocabul√°rio: Stemming e Lematiza√ß√£o\n",
        "\n",
        "Reduzir o vocabul√°rio √© essencial para evitar que o modelo trate varia√ß√µes de uma mesma palavra (como \"estudar\", \"estudando\", \"estudante\") como entidades distintas, o que pode levar ao sobreajuste (overfitting).\n",
        "\n",
        "| T√©cnica | Objetivo |\n",
        "| :--- | :--- |\n",
        "| **Stemming** | Remove afixos gramaticais (sufixos/prefixos), frequentemente resultando em uma \"raiz\" que pode n√£o ser uma palavra real (e.g., *organi* de organiza√ß√£o). |\n",
        "| **Lematiza√ß√£o** | Reduz a palavra √† sua forma de dicion√°rio (lema), que √© uma palavra v√°lida (e.g., *organiza√ß√£o* para organiza√ß√£o ou *correr* para correndo). |\n",
        "\n",
        "Geralmente, a lematiza√ß√£o √© mais sofisticada, pois utiliza informa√ß√µes de contexto (POS tagging) para garantir que a palavra base seja morfologicamente correta."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package rslp to\n",
            "[nltk_data]     /Users/rooneycoelho/nltk_data...\n",
            "[nltk_data]   Unzipping stemmers/rslp.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('rslp')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pt-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-3.8.0/pt_core_news_sm-3.8.0-py3-none-any.whl (13.0 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
            "\u001b[?25hInstalling collected packages: pt-core-news-sm\n",
            "Successfully installed pt-core-news-sm-3.8.0\n",
            "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('pt_core_news_sm')\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download pt_core_news_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"pt_core_news_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rw-2JNZxCOC",
        "outputId": "8eeafb2b-0f31-407c-fb01-bec19c4399b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokens em min√∫sculas (Lowercasing):\n",
            " ['aula', '1', '-', 'exemplo', 'de', 'pln', 'o', 'pln', '√©', 'uma', '√°rea', 'cr√≠tica', '!', 'h√°', 'muitos', 'dados', 'textuais', '(', 'e.g.', ',', 'tweets', ',', 'coment√°rios', ')', '.', 'a', 'tokeniza√ß√£o', '(', 'tokenizer', ')', '√©', 'a', 'primeira', 'etapa', '.', 'n√£o', 'se', 'esque√ßa', 'de', 'lematiza√ß√£o', '.', '√©', 'f√°cil', 'sobreajustar', 'o', 'modelo', '(', 'overfitting', ')', '.']\n",
            "\n",
            "Lemas (Portugu√™s):\n",
            " ['aular', '1', '-', 'exemplo', 'de', 'pln', 'o', 'pln', 'ser', 'um', '√°rea', 'cr√≠tico', '!', 'haver', 'muito', 'dado', 'textual', '(', 'e.g.', ',', 'tweet', ',', 'coment√°rio', ')', '.', 'o', 'tokeniza√ß√£o', '(', 'tokenizer', ')', 'ser', 'o', 'primeiro', 'etapa', '.', 'n√£o', 'se', 'esquecer', 'de', 'lematiza√ß√£o', '.', 'ser', 'f√°cil', 'sobreajustar', 'o', 'modelo', '(', 'overfitting', ')', '.']\n",
            "\n",
            "Tokens ap√≥s Stemming (Portugu√™s):\n",
            " ['aul', '1', '-', 'exempl', 'de', 'pln', 'o', 'pln', '√©', 'uma', '√°re', 'cr√≠t', '!', 'h√°', 'muit', 'dad', 'text', '(', 'e.g.', ',', 'tweet', ',', 'coment', ')', '.', 'a', 'token', '(', 'tokeniz', ')', '√©', 'a', 'prim', 'etap', '.', 'n√£o', 'se', 'esque√ß', 'de', 'lemat', '.', '√©', 'f√°cil', 'sobreajust', 'o', 'model', '(', 'overfitting', ')', '.']\n",
            "\n",
            "Nota: Lematiza√ß√£o seria aplicada aqui para obter a forma de dicion√°rio (lema).\n"
          ]
        }
      ],
      "source": [
        "# Simula√ß√£o de Redu√ß√£o de Vocabul√°rio usando NLTK (para fins did√°ticos)\n",
        "\n",
        "# Exemplo de Lowercasing (Convers√£o para min√∫sculas)\n",
        "# Converter todo o texto para min√∫sculas √© uma boa pr√°tica de normaliza√ß√£o,\n",
        "# pois em muitos casos, o caso n√£o √© relevante para o problema (e.g., classifica√ß√£o de not√≠cias).\n",
        "tokens_lower = [t.lower() for t in tokens]\n",
        "print(\"Tokens em min√∫sculas (Lowercasing):\\n\", tokens_lower)\n",
        "\n",
        "# Exemplo de Stemming (usando RSLP Stemmer para Portugu√™s, se dispon√≠vel, ou uma simula√ß√£o)\n",
        "# Como nossos tokens est√£o em Portugu√™s e o ambiente PySpark/SparkNLP √© frequentemente preferido,\n",
        "# usamos a implementa√ß√£o de conceitos.\n",
        "# Exemplo te√≥rico de Stemming:\n",
        "# from nltk.stem import PorterStemmer # Para Ingl√™s\n",
        "# stemmer = PorterStemmer()\n",
        "# stems = [stemmer.stem(t) for t in tokens_lower]\n",
        "# print(\"\\nExemplo de Stems (ingl√™s, apenas demonstra√ß√£o):\", stems)\n",
        "\n",
        "lemma = [t.lemma_ for t in nlp(\" \".join(tokens_lower))]\n",
        "print(\"\\nLemas (Portugu√™s):\\n\", lemma)\n",
        "\n",
        "stemmer = nltk.stem.RSLPStemmer()\n",
        "stems = [stemmer.stem(t) for t in tokens_lower]\n",
        "print(\"\\nTokens ap√≥s Stemming (Portugu√™s):\\n\", stems)\n",
        "\n",
        "# Exemplo de Lematiza√ß√£o (conceitual)\n",
        "# LemmatizationModel.pretrained() √© frequentemente usado em bibliotecas profissionais (e.g., Spark NLP).\n",
        "print(\"\\nNota: Lematiza√ß√£o seria aplicada aqui para obter a forma de dicion√°rio (lema).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLgLXBI8xCOC"
      },
      "source": [
        "### 5.2 Normaliza√ß√£o Adicional: Stop Words\n",
        "\n",
        "Outra pr√°tica essencial de normaliza√ß√£o √© a **Remo√ß√£o de Stop Words**, que s√£o palavras muito frequentes que n√£o carregam significado de conte√∫do √∫til para o problema (e.g., \"a\", \"an\", \"o\", \"de\", \"em\")."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgzB_CMHxCOC",
        "outputId": "03d5e3e8-588f-496f-8c13-21da50273b89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokens ap√≥s remo√ß√£o de Stop Words:\n",
            " ['aula', '1', 'exemplo', 'pln', 'pln', '√°rea', 'cr√≠tica', 'h√°', 'muitos', 'dados', 'textuais', 'tweets', 'coment√°rios', 'tokeniza√ß√£o', 'tokenizer', 'primeira', 'etapa', 'n√£o', 'se', 'esque√ßa', 'lematiza√ß√£o', 'f√°cil', 'sobreajustar', 'modelo', 'overfitting']\n"
          ]
        }
      ],
      "source": [
        "# Defini√ß√£o de Stop Words (exemplo simulado)\n",
        "\n",
        "# (Em um Colab real, voc√™ precisaria baixar o corpus de stopwords)\n",
        "# nltk.download('stopwords')\n",
        "\n",
        "stop_words_list = ['a', 'o', '√©', 'e', 'de', 'do', 'da', 'uma', 'um'] # Exemplo simples\n",
        "\n",
        "# Remo√ß√£o de Stop Words\n",
        "# Usamos .isalnum() para remover tamb√©m pontua√ß√µes que foram tokenizadas (como '!' e '.')\n",
        "tokens_filtrados = [word for word in tokens_lower if word not in stop_words_list and word.isalnum()]\n",
        "\n",
        "print(\"Tokens ap√≥s remo√ß√£o de Stop Words:\\n\", tokens_filtrados)\n",
        "\n",
        "# Demonstra√ß√£o de Corpus de Stop Words em Ingl√™s, frequentemente usado em PLN:\n",
        "# stop_words_en = stopwords.words('english')\n",
        "# print(f\"\\nN√∫mero de Stop Words em Ingl√™s (NLTK): {len(stop_words_en)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mB81-ADqxCOD"
      },
      "source": [
        "## 7\\. Conclus√£o\n",
        "\n",
        "O pipeline de pr√©-processamento (Limpeza, Tokeniza√ß√£o, Normaliza√ß√£o e Redu√ß√£o de Vocabul√°rio) √© essencial para preparar o texto para a modelagem. Uma vez que o texto √© transformado em vetores (Representa√ß√µes Discretas como BoW ou TF-IDF), podemos alimentar algoritmos cl√°ssicos de Machine Learning, como o Classificador Naive Bayes, que √© frequentemente usado como modelo baseline (linha de base) em classifica√ß√£o de texto devido √† sua rapidez e robustez.\n",
        "\n",
        "O Naive Bayes, contudo, baseia-se na suposi√ß√£o ing√™nua de que todas as palavras no documento s√£o mutuamente independentes e que a ordem das palavras n√£o importa, o que √© uma limita√ß√£o crucial. Isso motiva a pr√≥xima aula a explorar representa√ß√µes mais densas e ricas em contexto, como os Word Embeddings (Word2vec, GloVe), que capturam melhor o significado sem√¢ntico."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
