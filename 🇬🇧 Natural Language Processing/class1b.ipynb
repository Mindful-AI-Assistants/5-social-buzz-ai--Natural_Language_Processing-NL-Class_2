{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbuJmAtlyW89"
      },
      "source": [
        "# Representações Discretas de Texto (BoW, N-gram, TF-IDF)\n",
        "\n",
        "## 1\\. Introdução: Representando a Linguagem\n",
        "\n",
        "No Processamento de Linguagem Natural (PLN), um dos principais desafios é como converter o significado simbólico da linguagem humana (texto) em uma forma que possa ser compreendida por algoritmos de Machine Learning (ML). Essa conversão é chamada de **vetorização**.\n",
        "\n",
        "O princípio central para isso é o **Modelo de Espaço Vetorial (Vector Space Model - VSM)**, que define a vetorização como o processo de representar documentos como vetores de features (características).\n",
        "\n",
        "No contexto dos modelos clássicos, a representação de texto envolve a criação de vetores discretos (Sparse).\n",
        "\n",
        "Dado um vocabulário total $V$ de um corpus (a coleção inteira de documentos), o vetor $v_d$ de um documento $d$ terá a dimensão $|V|$. Como um documento individual contém apenas uma pequena fração das palavras do vocabulário total, a maioria dos valores no vetor $v_d$ será zero, resultando em **vetores esparsos**.\n",
        "\n",
        "## 2\\. Bag-of-Words (BoW): O Saco de Palavras\n",
        "\n",
        "O Bag-of-Words (BoW), ou Saco de Palavras, é a técnica mais direta e fundamental de representação vetorial em PLN.\n",
        "\n",
        "### 2.1 Conceito e Mecanismo\n",
        "\n",
        "  * **Definição:** O BoW trata o texto como uma \"bolsa\" ou coleção de palavras, **ignorando completamente a ordem das palavras** e a estrutura gramatical ou sintática. É um multiconjunto (multiset) onde cada elemento (palavra) tem uma contagem.\n",
        "  * **Vetorização:** Cada palavra única no conjunto de dados (o vocabulário) corresponde a uma dimensão (feature index) no vetor. O vetor do documento armazena a frequência de ocorrência de cada palavra nesse documento.\n",
        "\n",
        "**Exemplos de Representação BoW (Variações):**\n",
        "\n",
        "| Variação | Valor do Vetor | Fonte |\n",
        "| :--- | :--- | :--- |\n",
        "| **BoW de Contagem** | A contagem (frequência) da palavra no documento. | Mais popular |\n",
        "| **BoW Booleano** | 1 se a palavra estiver presente, 0 caso contrário. | Menos comum |\n",
        "\n",
        "### 2.2 Implementação Conceitual (Python Puro)\n",
        "\n",
        "Podemos simular a ideia do BoW usando um simples contador."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5yEETwayW9A",
        "outputId": "c600a905-b9e7-48af-8d5f-39b9ab9ec826"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Contagem de Palavras (BoW):\n",
            " Counter({'gato': 2, 'cartola': 2, 'o': 1, 'na': 1, 'e': 1, 'a': 1, 'do': 1})\n"
          ]
        }
      ],
      "source": [
        "# Demonstração Conceitual de BoW (Contagem)\n",
        "from collections import Counter\n",
        "\n",
        "texto = \"o gato na cartola e a cartola do gato\"\n",
        "tokens = texto.split() # Tokenização simplificada\n",
        "counts = Counter(tokens)\n",
        "\n",
        "print(\"Contagem de Palavras (BoW):\\n\", counts)\n",
        "\n",
        "# Vocabulário (mapeamento de palavra para índice):\n",
        "# {'o': 0, 'gato': 1, 'na': 2, 'cartola': 3, 'e': 4, 'a': 5, 'do': 6}\n",
        "# Vetor (simulado): [1, 2, 1, 2, 1, 1, 1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x03FbSByyW9B"
      },
      "source": [
        "### 2.3 Implementação Prática com `scikit-learn`\n",
        "\n",
        "Em um pipeline real, usamos `CountVectorizer` do `scikit-learn` para criar a matriz Documento-Termo (Document-Term Matrix)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "id": "EqYtPwjEyW9C",
        "outputId": "ef107e1f-f9cc-48ab-f704-526bcda81da2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulário (Features):\n",
            "['cachorro' 'cartola' 'gato']\n",
            "\n",
            "Matriz Esparsa (Documento, Termo):\n",
            "[[0 1 1]\n",
            " [0 1 1]\n",
            " [1 0 1]]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cachorro</th>\n",
              "      <th>cartola</th>\n",
              "      <th>gato</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Doc 1</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Doc 2</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Doc 3</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       cachorro  cartola  gato\n",
              "Doc 1         0        1     1\n",
              "Doc 2         0        1     1\n",
              "Doc 3         1        0     1"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Implementação Prática com CountVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "# Nosso \"corpus\" (coleção de documentos)\n",
        "corpus = [\n",
        "    'o gato na cartola',\n",
        "    'a cartola do gato',\n",
        "    'o gato e o cachorro'\n",
        "]\n",
        "\n",
        "# 1. Inicializar o vetorizador\n",
        "# (vamos incluir stop_words em português para uma limpeza básica)\n",
        "vectorizer_bow = CountVectorizer(stop_words=['o', 'na', 'e', 'a', 'do', 'da'])\n",
        "\n",
        "# 2. Aprender o vocabulário e transformar o corpus\n",
        "X_bow = vectorizer_bow.fit_transform(corpus)\n",
        "\n",
        "# 3. Visualizar o resultado\n",
        "print(\"Vocabulário (Features):\")\n",
        "print(vectorizer_bow.get_feature_names_out())\n",
        "print(\"\\nMatriz Esparsa (Documento, Termo):\")\n",
        "print(X_bow.toarray())\n",
        "\n",
        "# Visualização como DataFrame para clareza\n",
        "pd.DataFrame(X_bow.toarray(), columns=vectorizer_bow.get_feature_names_out(), index=['Doc 1', 'Doc 2', 'Doc 3'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkmivGI5yW9C"
      },
      "source": [
        "### 2.4 Desvantagens e Limitações do BoW\n",
        "\n",
        "O BoW, embora simples, apresenta limitações críticas:\n",
        "\n",
        "1.  **Visão Plana do Documento (Flattened View):** O BoW perde completamente a ordem e a estrutura sintática.\n",
        "      * `\"o cavalo comeu\"` e `\"comeu o cavalo\"` teriam o **mesmo vetor** BoW.\n",
        "2.  **Maldição da Dimensionalidade (Curse of Dimensionality):** Se o vocabulário for muito grande (e.g., \\> 100.000 palavras), o vetor terá milhares de dimensões, sendo a maioria zeros (esparsidade).\n",
        "3.  **Similaridade Semântica (Ortogonalidade):** Não há conceito de similaridade de significado. No espaço BoW, a distância entre palavras sinônimas é a mesma que entre palavras não relacionadas.\n",
        "      * $d(\\text{cão, gato}) = d(\\text{cão, cadeira})$\n",
        "\n",
        "## 3\\. N-grams: Adicionando Contexto Local\n",
        "\n",
        "A abordagem BoW falha em incorporar a ordem das palavras. Os N-grams (ou Bag-of-N-Grams - BoN) são usados para resolver parcialmente esta questão, capturando algum contexto local.\n",
        "\n",
        "### 3.1 Conceito e Tipos\n",
        "\n",
        "  * **Definição:** Um N-gram é uma subsequência de $N$ palavras contíguas.\n",
        "  * **Mecanismo:** O vocabulário $V$ é estendido para incluir todos os $N$-grams únicos. O vetor do documento armazena as contagens desses $N$-grams.\n",
        "  * **Tipos Comuns:**\n",
        "      * **Unigramas (N=1):** Equivale ao BoW. (`\"gato\"`, `\"cartola\"`)\n",
        "      * **Bigramas (N=2):** Combinações de duas palavras. (`\"gato na\"`, `\"na cartola\"`)\n",
        "      * **Trigramas (N=3):** Combinações de três palavras. (`\"gato na cartola\"`)\n",
        "\n",
        "### 3.2 Dimensionalidade e Escolha de N\n",
        "\n",
        "  * **Aumento da Esparsidade:** Usar $N$ maior (e.g., trigramas) captura mais contexto, mas aumenta *exponencialmente* o tamanho do vocabulário e a esparsidade do vetor.\n",
        "  * **Escolha de N:** Geralmente $N < 4$. Para textos curtos (tweets), bigramas são comuns. Para documentos mais longos, uma combinação de unigramas e bigramas (`ngram_range=(1, 2)`) é um ótimo ponto de partida.\n",
        "\n",
        "### 3.3 Implementação Prática com `scikit-learn`\n",
        "\n",
        "Não precisamos de um loop manual. O `CountVectorizer` lida com N-grams através do parâmetro `ngram_range`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "id": "PdJ22qqAyW9C",
        "outputId": "34462ff0-ca45-4238-c2cf-ddadc4abee20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulário (Unigramas + Bigramas):\n",
            "['cachorro' 'cartola' 'cartola gato' 'gato' 'gato cachorro' 'gato cartola']\n",
            "\n",
            "Matriz Esparsa (Documento, N-gram):\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cachorro</th>\n",
              "      <th>cartola</th>\n",
              "      <th>cartola gato</th>\n",
              "      <th>gato</th>\n",
              "      <th>gato cachorro</th>\n",
              "      <th>gato cartola</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Doc 1</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Doc 2</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Doc 3</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       cachorro  cartola  cartola gato  gato  gato cachorro  gato cartola\n",
              "Doc 1         0        1             0     1              0             1\n",
              "Doc 2         0        1             1     1              0             0\n",
              "Doc 3         1        0             0     1              1             0"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Implementação Prática de N-grams (Bigramas)\n",
        "\n",
        "# Vamos usar o mesmo corpus, mas agora pedindo unigramas E bigramas\n",
        "# ngram_range=(1, 2) significa \"incluir N=1 e N=2\"\n",
        "vectorizer_ngram = CountVectorizer(stop_words=['o', 'na', 'e', 'a', 'do', 'da'], ngram_range=(1, 2))\n",
        "\n",
        "# Aprender o vocabulário e transformar\n",
        "X_ngram = vectorizer_ngram.fit_transform(corpus)\n",
        "\n",
        "# Visualizar o resultado\n",
        "print(\"Vocabulário (Unigramas + Bigramas):\")\n",
        "print(vectorizer_ngram.get_feature_names_out())\n",
        "\n",
        "print(\"\\nMatriz Esparsa (Documento, N-gram):\")\n",
        "pd.DataFrame(X_ngram.toarray(), columns=vectorizer_ngram.get_feature_names_out(), index=['Doc 1', 'Doc 2', 'Doc 3'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mBQZX_ayW9D"
      },
      "source": [
        "## 4\\. TF-IDF: Ponderando a Relevância do Termo\n",
        "\n",
        "O TF-IDF (Term Frequency-Inverse Document Frequency) resolve uma deficiência central do BoW de contagem: palavras muito comuns (como \"o\", \"de\", \"que\") dominam a contagem, mesmo sem carregar significado discriminativo.\n",
        "\n",
        "O TF-IDF quantifica a **importância** de uma palavra combinando dois fatores:\n",
        "\n",
        "1.  **Frequência do Termo (Term Frequency - TF):** Quão frequente é a palavra *neste documento*?\n",
        "2.  **Frequência Inversa do Documento (Inverse Document Frequency - IDF):** Quão *rara* é a palavra *em todo o corpus*?\n",
        "\n",
        "### 4.1 Formulação Matemática\n",
        "\n",
        "O TF-IDF é o produto dos dois.\n",
        "\n",
        "#### 4.1.1 Term Frequency (TF)\n",
        "\n",
        "O TF($w_i, d$) é o número de vezes que a palavra $w_i$ aparece no documento $d$. (Frequentemente normalizado ou suavizado logaritmicamente).\n",
        "\n",
        "$$\n",
        "\\text{TF}(w_i, d) = f_{w_i, d}\n",
        "$$#### 4.1.2 Inverse Document Frequency (IDF)\n",
        "\n",
        "O IDF($w_i$) atribui um peso maior a termos que são raros no corpus. Palavras que aparecem em *todos* os documentos (como stopwords) terão um IDF próximo de zero.\n",
        "\n",
        "$$\\text{IDF}(w\\_i) = \\log \\left( \\frac{N}{\\text{df}(w_i)} \\right)\n",
        "$$Onde $N$ é o número total de documentos no corpus e $\\text{df}(w_i)$ (Document Frequency) é o número de documentos que contêm a palavra $w_i$.\n",
        "\n",
        "*(Nota: O `scikit-learn` usa uma versão suavizada \"smooth\" por padrão, adicionando +1 ao numerador e denominador para evitar divisão por zero).*\n",
        "\n",
        "#### 4.1.3 TF-IDF Score\n",
        "\n",
        "O score final equilibra a frequência local (TF) com a raridade global (IDF).\n",
        "\n",
        "$$\n",
        "\\text{TFIDF}(w_i, d) = \\text{TF}(w_i, d) \\times \\text{IDF}(w_i)\n",
        "$$Um valor **alto** de TF-IDF indica que o termo é frequente *naquele documento*, mas raro *no corpus total*, tornando-o um bom discriminador.\n",
        "\n",
        "### 4.2 Implementação Prática com `scikit-learn`\n",
        "\n",
        "Usamos o `TfidfVectorizer`, que combina o `CountVectorizer` com a transformação TF-IDF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "id": "_TmGmxOJyW9E",
        "outputId": "b8c18d0b-2ee8-47e3-fc34-c41460651ff8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulário (Features):\n",
            "['bandeco' 'bom' 'cachorro' 'cartola' 'churrasco' 'gato' 'puc']\n",
            "\n",
            "Matriz TF-IDF (Documento, Feature):\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>bandeco</th>\n",
              "      <th>bom</th>\n",
              "      <th>cachorro</th>\n",
              "      <th>cartola</th>\n",
              "      <th>churrasco</th>\n",
              "      <th>gato</th>\n",
              "      <th>puc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Doc 1</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.833884</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.551939</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Doc 2</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.833884</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.551939</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Doc 3</th>\n",
              "      <td>0.351597</td>\n",
              "      <td>0.351597</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.351597</td>\n",
              "      <td>0.366956</td>\n",
              "      <td>0.703194</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Doc 4</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.886548</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.462637</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        bandeco       bom  cachorro   cartola  churrasco      gato       puc\n",
              "Doc 1  0.000000  0.000000  0.000000  0.833884   0.000000  0.551939  0.000000\n",
              "Doc 2  0.000000  0.000000  0.000000  0.833884   0.000000  0.551939  0.000000\n",
              "Doc 3  0.351597  0.351597  0.000000  0.000000   0.351597  0.366956  0.703194\n",
              "Doc 4  0.000000  0.000000  0.886548  0.000000   0.000000  0.462637  0.000000"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Implementação Prática com TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Nosso \"corpus\" (coleção de documentos)\n",
        "#corpus = [\n",
        "#    'o gato na cartola',\n",
        "#    'a cartola do gato',\n",
        "#    'o gato e o cachorro'\n",
        "#]\n",
        "\n",
        "# Nosso \"corpus\" (coleção de documentos)\n",
        "corpus = [\n",
        "    'o gato na cartola',\n",
        "    'a cartola do gato',\n",
        "    'o gato na PUC. Churrasco de gato do bandeco na PUC é bom!',\n",
        "    'o gato e o cachorro'\n",
        "]\n",
        "\n",
        "# Usando o mesmo corpus\n",
        "# (O TfidfVectorizer também pode calcular N-grams e remover stop_words)\n",
        "#tfidf_vectorizer = TfidfVectorizer(stop_words=['o', 'na', 'e', 'a', 'do', 'da', 'de'])\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words=['o', 'na', 'e', 'a', 'do', 'da', 'de'])\n",
        "\n",
        "# Aprender vocabulário, contar e aplicar a transformação TF-IDF\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Visualizar o resultado\n",
        "print(\"Vocabulário (Features):\")\n",
        "print(tfidf_vectorizer.get_feature_names_out())\n",
        "\n",
        "print(\"\\nMatriz TF-IDF (Documento, Feature):\")\n",
        "# Note que os valores agora são pesos (floats), não contagens (inteiros)\n",
        "pd.DataFrame(X_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out(), index=['Doc 1', 'Doc 2', 'Doc 3', 'Doc 4'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TastDn2MyW9E"
      },
      "source": [
        "*Observação: Note como \"gato\" e \"cartola\", que aparecem em 2 de 3 documentos, têm um peso menor do que termos únicos de um documento, como \"cachorro\".*\n",
        "\n",
        "### 4.3 Uso e Limitações do TF-IDF\n",
        "\n",
        "O TF-IDF é um *baseline* (linha de base) extremamente forte para tarefas de classificação de texto (e.g., análise de sentimento, classificação de notícias) quando usado com algoritmos clássicos (Naive Bayes, SVM, Regressão Logística).\n",
        "\n",
        "Contudo, o TF-IDF herda as principais fraquezas da representação Bag-of-Words:\n",
        "\n",
        "* **Insensibilidade ao Contexto:** A ordem das palavras ainda é perdida (a menos que se use N-grams, que captura apenas contexto local).\n",
        "* **Maldição da Dimensionalidade:** Ainda sofre com vetores longos e esparsos.\n",
        "* **Ortogonalidade:** Ainda não há noção de similaridade semântica (e.g., \"cão\" e \"cachorro\" são features completamente diferentes).\n",
        "\n",
        "## 5\\. Próximos Passos: Indo Além das Representações Discretas\n",
        "\n",
        "As representações BoW, N-gram e TF-IDF são modelos baseados em contagem que criam vetores **discretos e esparsos**.\n",
        "\n",
        "As limitações inerentes a esses métodos—especialmente a perda de contexto/ordem e a falta de similaridade semântica—motivaram o desenvolvimento de modelos mais avançados.\n",
        "\n",
        "O próximo passo é a transição para **Representações Distribuídas (Distributed Representations)**, também conhecidas como **Word Embeddings**. Esses modelos (como Word2vec, GloVe e BERT) usam vetores **densos** (com poucas dimensões, e.g., 300) para capturar o significado semântico e contextual das palavras.\n",
        "$$"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
